#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# PYTHON_ARGCOMPLETE_OK

"""
@file     urls_organizer.py
@date     2023-05-01
@version  1.1 (Stable)
@license  GNU General Public License v3.0
@url      https://github.com/yisuschrist/urls_organizer
@author   Alejandro Gonzalez Momblan (agelrenorenardo@gmail.com)
@desc     Organize your URL saved

@details  This script is intended to be used with Python 3.6 or greater.
          It may work with previous versions but it is not guaranteed.

@requires Python 3.6 or greater
          requests
          validators
          natsort
          tqdm

@todo     Add support for arguments completion with argcomplete.
          Reference: https://pypi.org/project/argcomplete/#global-completion

"""
import argparse
import hashlib
import logging
import os
import signal
import sys
from functools import lru_cache
from multiprocessing import Manager, Pool

sys.path.append(os.path.abspath(".."))

import requests  # pip install requests
import validators  # pip install validators
from exitstatus import ExitStatus  # pip install exitstatus
from natsort import natsorted  # pip install natsort
from personal_utils import *
from tqdm import tqdm  # pip install tqdm

MULTIPROCESSING_THREADS = os.cpu_count() * 2
FILE = os.path.basename(__file__)
CACHE_PATH = os.path.expanduser(f"~/.cache/{FILE}")  #! TODO: Fix wrong path
LOG_FILE = f"/tmp/log/{FILE}.log"
INVALID_URLS_FILE = "invalid_urls.txt"

manager = Manager()
invalid_url_list = manager.list()
parser = None


def get_version():
    """
    Return the version of the program.

    Returns:
        str: The version of the program.
    """
    try:
        # Open the file and read its contents.
        with open(FILE) as f:
            content = f.read()
        return grab(content, "@version", "\n").strip()
    except:
        logging.error("Could not find version")


def init_worker() -> None:
    """
    Initialize the worker process.

    This function sets the signal handling for the worker process to ignore
    the `SIGINT` signal.
    """
    signal.signal(signal.SIGINT, signal.SIG_IGN)


def normalize_url(url: str) -> str:
    """
    Remove unnecessary URL extensions from a given string.

    Args:
        url (str): The string to remove the extensions from.

    Returns:
        str: The input string with unnecessary URL extensions removed.
    """
    # Can't remove / from end of URL because it is used to identify directories
    if url.endswith("#"):
        url = url[:-1]

    return (
        url.replace("/www.", "/")
        .replace("/m.", "/")
        .replace("/es.", "/")
        .replace("http:", "https:")
    )


def parse_url(url: str) -> str:
    """
    Parse a URL string and remove unnecessary parts.

    Args:
        url (str): The URL to parse.

    Returns:
        str: The parsed URL with unnecessary parts removed.
    """
    logging.debug("Parsing URL: %s" % url)

    # List of URL extensions to remove
    extension_rules = [
        "&",
        "?",
        "/?",
        "/#",
    ]
    keep_extension = "viewkey"

    # Remove whitespaces to avoid errors
    url = url.strip()

    # Remove URL additional media data
    if keep_extension not in url:
        for r in extension_rules:
            url = url.split(r, 1)[0]

    # Keep the ?viewkey parameter and remove other query parameters
    url_parts = url.split("?")
    if len(url_parts) > 1:
        base_url = url_parts[0]
        query_params = url_parts[1].split("&")
        keep_params = [p for p in query_params if p.startswith(keep_extension)]
        url = base_url + "?" + "&".join(keep_params)

    # Remove unnecessary URL extensions
    return normalize_url(url)


@lru_cache()
def validate_url(url: str) -> None:
    """
    Validate the given URL by making a GET request and checking the response status code.

    Args:
        url (str): The URL to validate.

    Raises:
        HTTPError: If the response status code is not 200.
        ConnectionError: If there was an error connecting to the URL.
        OSError: If there was an error making the GET request.
    """
    logging.debug("Validating URL: <%s>" % url)

    # Remove the URL title if it exists
    url = url.split(" (")[0]

    # check if the link is in the cache
    url_hash = hashlib.md5(url.encode("utf-8")).hexdigest()
    filename = os.path.join(CACHE_PATH, url_hash)
    if os.path.exists(filename):
        logging.warning("Link %s already visited, skipping...", url)
        return

    # Make a GET request
    try:
        if not validators.url(url):
            raise requests.exceptions.InvalidURL

        #! TODO: Fix when some pages return code != 200 but the page is valid
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        # save to cache
        cache_file = os.path.join(CACHE_PATH, url_hash)
        with open(cache_file, "w", encoding="utf-8"):
            pass

        return

    except requests.exceptions.HTTPError as e:
        logging.info(
            "Invalid URL: %s with status code %s" % (url, response.status_code)
        )
    except requests.exceptions.InvalidURL:
        logging.error("Invalid URL: %s" % url)
    except requests.exceptions.ConnectionError:
        logging.error("Error connecting to %s" % url)
    except OSError:
        logging.error("Error making GET request to %s" % url)
    except Exception as e:
        logging.exception(e)

    invalid_url_list.append(url)


def validate_url_list(args: argparse.Namespace, url_list: list):
    """
    Validate a list of URLs using multiple processes and display the number
    of invalid URLs found.

    Args:
        args (argparse.Namespace): Namespace object containing command-line
                                   arguments.
        url_list (list): List of URLs to be validated.
    """
    # Calculate optimal number of processes based on system capacity
    optimal_processes = min(MULTIPROCESSING_THREADS, len(url_list))
    num_processes = min(args.numWorkers, optimal_processes)
    if num_processes < args.numWorkers:
        logging.warning(
            f"Warning: numWorkers set to {args.numWorkers}, which is above "
            f"the optimal value of {MULTIPROCESSING_THREADS}. Using "
            f"{num_processes} workers instead."
        )

    try:
        with Pool(processes=num_processes, initializer=init_worker) as pool:
            for _ in tqdm(
                pool.imap_unordered(validate_url, url_list),
                total=len(url_list),
            ):
                pass
    except KeyboardInterrupt:
        logging.info("Caught KeyboardInterrupt, terminating workers")
        pool.terminate()
        pool.join()

    logging.info(
        f"Found {len(invalid_url_list)} invalid URLs out of {len(url_list)}"
    )


def get_urls_from_file(file_path: str) -> list:
    """
    Read a file and return a list of URLs after parsing.

    Args:
        file (str): The path to the file to read.

    Returns:
        list: A list of URLs after parsing.
    """
    logging.debug("Reading URLs from file: %s" % file_path)
    try:
        # Open the file with utf-8 encoding and parse each line as a URL
        content = read_file(file_path)
        return [parse_url(line) for line in content]
    except FileNotFoundError as e:
        # If the file is not found, log an error and exit the program with an error code
        logging.error(e)
        exit_session(ExitStatus.failure)


def save_urls_to_file(url_list: list, file_path: str) -> None:
    """
    Save a list of URLs to a file.

    Args:
        url_list (list): The list of URLs to save.
        file_path (str): The path of the file to save to.
    """
    logging.debug("Saving URLs to file: %s" % file_path)
    try:
        write_file(file_path, "\n".join(url_list))
    except FileNotFoundError as e:
        # If the file is not found, log an error and exit the program with an error code
        logging.error(e)
        exit_session(ExitStatus.failure)


def merge_content(data: list, file_path: str) -> list:
    """
    Merge a list of strings with the contents of a file, removing duplicates.

    Args:
        data (list): The list of strings to merge with the file.
        file_path (str): The path of the file to merge with.

    Returns:
        list: The merged list with duplicates removed.
    """
    file_data = get_urls_from_file(file_path)

    # Remove duplicates from the merged data
    merged_data = remove_duplicates(data + file_data)
    # Sort the merged data in natural order
    return natsorted(merged_data)


def check_positive(value: str) -> int:
    """
    Check if a given value is a positive integer and return it.

    Args:
        value (str): The value to check.

    Returns:
        int: The input value as an integer if it is a positive integer.

    Raises:
        argparse.ArgumentTypeError: If the input value is not a positive integer.
    """
    ivalue = int(value)
    if ivalue <= 0:
        raise argparse.ArgumentTypeError("%s is not a valid value" % value)
    return ivalue


def exit_session(exit_value: int) -> None:
    """
    Exit the program with the given exit value.

    Args:
        exit_value (int): The POSIX exit value to exit with.
    """
    logging.info("End of session")
    # Check if the exit_value is a valid POSIX exit value
    if not 0 <= exit_value <= 255:
        exit_value = ExitStatus.failure

    # Exit the program with the given exit value
    sys.exit(exit_value)


def get_parsed_args() -> argparse.Namespace:
    """
    Parse and return command-line arguments.

    Returns:version(__file__)
        The parsed arguments as an argparse.Namespace object.
    """
    global parser

    parser = argparse.ArgumentParser(
        description="Organize your URL saved",  # Program description
        formatter_class=argparse.RawTextHelpFormatter,  # Disable line wrapping
        allow_abbrev=False,  # Disable abbreviations
        add_help=False,  # Disable default help
    )

    g_targets = parser.add_argument_group("Options to add URLs")
    g_targets.add_argument(
        "-sf",
        "--saveFile",
        dest="saveFile",
        default=False,
        help="File with the URLs result. Argument is required",
    )
    g_targets.add_argument(
        "-rf",
        "--readFile",
        dest="readFile",
        default=False,
        help="File with the URLs to add. Argument is required if -u is not used.",
    )
    g_targets.add_argument(
        "-u",
        "--url",
        dest="url",
        default=False,
        help="Single URL to add. Argument is required if -rf is not used.",
    )
    g_targets.add_argument(
        "-w",
        "--numWorkers",
        dest="numWorkers",
        default=0,
        type=check_positive,
        help="Number of workers to use. Default is the number of CPU cores * 2.",
    )

    g_misc = parser.add_argument_group("Miscellaneous Options")
    # Help
    g_misc.add_argument(
        "-h", "--help", action="help", help="Show this help message and exit."
    )
    # Verbose
    g_misc.add_argument(
        "-t",
        "--verbose",
        dest="verbose",
        action="store_true",
        default=False,
        help="Show log messages on screen. Default is False.",
    )
    # Debug
    g_misc.add_argument(
        "-d",
        "--debug",
        dest="debug",
        action="store_true",
        default=False,
        help="Activate debug logs. Default is False.",
    )
    g_misc.add_argument(
        "-v",
        "--version",
        action="version",
        help="Show version number and exit.",
        version=f"%(prog)s version {get_version()}",
    )

    return parser.parse_args()


def setup_logger(
    args: argparse.Namespace,
    log_file: str = LOG_FILE,
) -> None:
    """
    Set up the logger for this instance.

    This method sets up the logger for the instance based on the `verbose`
    and `debug` command line arguments.

    Args:
        - args (argparse.Namespace): The command line arguments.
        - log_file (str): The file to log to. Default is
          /tmp/log/urls_organizer.log.
    """
    handlers = [logging.FileHandler(log_file)]
    level = logging.INFO
    msg_format = "[%(asctime)s] %(levelname)s: %(message)s"

    if args.verbose:
        handlers.append(logging.StreamHandler())

    if args.debug:
        level = logging.DEBUG
        msg_format += ": %(pathname)s:%(lineno)d in %(funcName)s"

    logging.basicConfig(
        level=level,
        format=msg_format,
        handlers=handlers,
    )


def main():
    """
    Main function of the script.
    """
    global parser, invalid_url_list

    args = get_parsed_args()
    setup_logger(args=args)

    if not args.saveFile or not (args.readFile or args.url):
        print("ERROR: No destination file or source file/URL specified\n")
        parser.print_help()
        exit_session(ExitStatus.failure)

    logging.info("Starting session...")
    ensure_exists(CACHE_PATH, INVALID_URLS_FILE, args.saveFile)

    if args.readFile:
        # Read URLs from file
        data = get_urls_from_file(args.readFile)

    else:
        # Read URL from command line
        data = [parse_url(args.url)]
        logging.debug(f"The new URL is {data[0]}")

    old_length = len(data)

    # Merge data
    data = merge_content(data, args.saveFile)

    new_length = len(data)

    if args.numWorkers > 0:
        validate_url_list(args, data)

    if invalid_url_list:
        logging.info(f"Invalid URLs found! Check the file {INVALID_URLS_FILE}")

        invalid_url_list = merge_content(invalid_url_list, INVALID_URLS_FILE)
        # Save the invalid URLs to file
        save_urls_to_file(invalid_url_list, INVALID_URLS_FILE)

        data = [u for u in data if u not in invalid_url_list]

    # Save the final collection URLs to file
    save_urls_to_file(data, args.saveFile)

    print("\nOperation finished!\n")
    logging.info(
        f"Found {old_length - new_length} duplicate URLs out of "
        f"{old_length} total URLs"
        if old_length > new_length
        else "No " "duplicate URLs found"
    )

    exit_session(ExitStatus.success)


if __name__ == "__main__":
    main()
